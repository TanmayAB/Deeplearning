{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tanmay Bhatt\n",
    "011499072\n",
    "CMPE 258\n",
    "Assignment 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from math import sqrt\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from __future__ import division\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 (10pts). Linear regression with one variable from scratch\n",
    "Using Jupyter notebook, load the data (ex1data1.csv). Visualize data using scatter plot.\n",
    "The first column is Population of City in 10,000s, and the second column is profit of food truck in 10,000.\n",
    "In order to predict the profit, fit the data using gradient descent method (without matrix). You need to calculate\n",
    "cost function and update weight using gradient descent method. Try several different learning rate. Please print\n",
    "Root Mean Squared Error (RMSE) after optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('./ex1data1.csv', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "97\n"
     ]
    }
   ],
   "source": [
    "X = data[0]\n",
    "Y = data[1]\n",
    "m = len(data)\n",
    "print m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(x):\n",
    "    return w0 + w1*x\n",
    "\n",
    "def calculate_cost(Y_pred):\n",
    "    result = 0\n",
    "    for y_pred,y in zip(Y_pred,Y):\n",
    "        result += (y_pred - y)**2\n",
    "    result /=m\n",
    "    return result\n",
    "\n",
    "def cost_derivative_0(Y_pred):\n",
    "    result = 0\n",
    "    for y_pred,y in zip(Y_pred,Y):\n",
    "        result += (y_pred - y)\n",
    "    result *=2\n",
    "    result /=m\n",
    "    return result\n",
    "\n",
    "def cost_derivative_1(Y_pred):\n",
    "    result = 0\n",
    "    for y_pred,y,x in zip(Y_pred,Y,X):\n",
    "        result += (y_pred - y) * x\n",
    "    result *=2\n",
    "    result /=m\n",
    "    return result\n",
    "\n",
    "def calculate_weights(rate,Y_pred):\n",
    "    global w0\n",
    "    global w1\n",
    "    w0 = w0 - rate * cost_derivative_0(Y_pred)\n",
    "    w1 = w1 - rate * cost_derivative_1(Y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('RMSE before gradient descent', 8.009086574317406)\n"
     ]
    }
   ],
   "source": [
    "w0 = 0\n",
    "w1 = 0\n",
    "count = 0\n",
    "learning_rates = [0.1,0.01,0.001,0.0001,0.00001]\n",
    "max_count = 1000\n",
    "Y_pred = []\n",
    "for x in X:\n",
    "    Y_pred.append(predict(x))\n",
    "print(\"RMSE before gradient descent\",sqrt(calculate_cost(Y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iterations : 1000\n",
      "Last Cost : 8.956041\n",
      "Second Last Cost : 8.956057 \n",
      "Learning rate : 0.010000 \n",
      "w0 : -3.788419 \n",
      "w1 : 1.182248\n",
      "RMSE : 2.992665\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for rate  in learning_rates:\n",
    "    w0 = 0\n",
    "    w1 = 0\n",
    "    count = 0\n",
    "    max_count = 1000\n",
    "    w0 = 0\n",
    "    Y_pred = []\n",
    "    for x in X:\n",
    "        Y_pred.append(predict(x))\n",
    "    current_cost = calculate_cost(Y_pred)\n",
    "    new_cost = 1\n",
    "\n",
    "    while new_cost < current_cost and count < max_count:\n",
    "        current_cost = calculate_cost(Y_pred)\n",
    "        calculate_weights(rate,Y_pred)\n",
    "        Y_pred = []\n",
    "        for x in X:\n",
    "            Y_pred.append(predict(x))\n",
    "        new_cost = calculate_cost(Y_pred)\n",
    "        count += 1\n",
    "    print \"Iterations : %d\" % count\n",
    "    print \"Last Cost : %f\" % new_cost\n",
    "    print \"Second Last Cost : %f \" % current_cost \n",
    "    print \"Learning rate : %f \" % rate\n",
    "    print \"w0 : %f \" % w0\n",
    "    print \"w1 : %f\" % w1\n",
    "    print \"RMSE : %f\" % sqrt(calculate_cost(Y_pred))\n",
    "    print \"\\n\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Best Result with Learning rate : 0.010000 \n",
    "\n",
    "Iterations : 1000 <br />\n",
    "Last Cost : 8.956041 <br />\n",
    "Second Last Cost : 8.956057 <br /> \n",
    "Learning rate : 0.010000 <br />\n",
    "w0 : -3.788419 <br />\n",
    "w1 : 1.182248 <br />\n",
    "RMSE : 2.992665 <br />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x113318b90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(data[0],data[1])\n",
    "plt.plot(data[0],Y_pred)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2(30pts). Linear regression with two variables from scratch\n",
    "Using Jupyter notebook, load the data (ex1data2.csv). Visualize data.\n",
    "The first column is the size of the house (in square feet), the second column is the number of bedrooms, and the\n",
    "third column is the price of the house.\n",
    "In order to predict the housing price, fit the data using gradient descent method (without matrix). You need to\n",
    "normalize variables. You need to calculate cost function and update weight using gradient descent method. Try\n",
    "several different learning rate. Please print the Root Mean Squared Error (RMSE) after optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('ex1data2.csv', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "X1 = data[0]\n",
    "X2 = data[1]\n",
    "Y = data[2]\n",
    "m = len(data)\n",
    "\n",
    "X1 = X1.astype(float) \n",
    "X2 = X2.astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "852.0\n",
      "4478.0\n"
     ]
    }
   ],
   "source": [
    "X1_min = X1.min()\n",
    "print (X1_min)\n",
    "X1_max = X1.max()\n",
    "print X1_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "X1_min = X1.min()\n",
    "X1_max = X1.max()\n",
    "X2_min = X2.min()\n",
    "X2_max = X2.max()\n",
    "i = 0\n",
    "for x1,x2 in zip(X1,X2):\n",
    "    X1[i] = ((x1 - X1_min)/(X1_max - X1_min))\n",
    "    X2[i] = ((x2 - X2_min)/(X2_max - X2_min))\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(x1,x2):\n",
    "    return w0 + w1*x1 + w2*x2\n",
    "\n",
    "def calculate_cost(Y_pred):\n",
    "    result = 0\n",
    "    for y_pred,y in zip(Y_pred,Y):\n",
    "        result += (y_pred - y)**2\n",
    "    result /=m\n",
    "    return result\n",
    "\n",
    "def cost_derivative_0(Y_pred):\n",
    "    result = 0\n",
    "    for y_pred,y in zip(Y_pred,Y):\n",
    "        result += (y_pred - y)\n",
    "    result *=2\n",
    "    result /=m\n",
    "    return result\n",
    "\n",
    "def cost_derivative_1(Y_pred):\n",
    "    result = 0\n",
    "    for y_pred,y,x1 in zip(Y_pred,Y,X1):\n",
    "        result += (y_pred - y) * x1\n",
    "    result *=2\n",
    "    result /=m\n",
    "    return result\n",
    "def cost_derivative_2(Y_pred):\n",
    "    result = 0\n",
    "    for y_pred,y,x2 in zip(Y_pred,Y,X2):\n",
    "        result += (y_pred - y) * x2\n",
    "    result *=2\n",
    "    result /=m\n",
    "    return result\n",
    "\n",
    "def calculate_weights(rate,Y_pred):\n",
    "    global w0\n",
    "    global w1\n",
    "    global w2\n",
    "    w0 = w0 - rate * cost_derivative_0(Y_pred)\n",
    "    w1 = w1 - rate * cost_derivative_1(Y_pred)\n",
    "    w2 = w2 - rate * cost_derivative_2(Y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('RMSE before gradient descent', 362192.17480415246)\n"
     ]
    }
   ],
   "source": [
    "w0 = 0\n",
    "w1 = 0\n",
    "w2 = 0\n",
    "count = 0\n",
    "learning_rates = [0.1,0.01,0.001,0.0001,0.00001,0.000001,0.0000001,0.00000000001]\n",
    "max_count = 1000\n",
    "Y_pred = []\n",
    "for x1,x2 in zip(X1,X2):\n",
    "    Y_pred.append(predict(x1,x2))\n",
    "print(\"RMSE before gradient descent\",sqrt(calculate_cost(Y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iterations : 1000\n",
      "Last Cost : 4089308608.459870\n",
      "Second Last Cost : 4089326941.073368 \n",
      "Learning rate : 0.100000 \n",
      "w0 : 196128.655623 \n",
      "w1 : 497677.618939\n",
      "w2 : -24754.055071\n",
      "RMSE : 63947.702136\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for rate  in learning_rates:\n",
    "    w0 = 0\n",
    "    w1 = 0\n",
    "    w2 = 0\n",
    "    count = 0\n",
    "    max_count = 1000\n",
    "    Y_pred = []\n",
    "    for x1,x2 in zip(X1,X2):\n",
    "        Y_pred.append(predict(x1,x2))\n",
    "    len(Y_pred)\n",
    "    current_cost = calculate_cost(Y_pred)\n",
    "    new_cost = 1\n",
    "\n",
    "    while new_cost < current_cost and count < max_count:\n",
    "        current_cost = calculate_cost(Y_pred)\n",
    "        calculate_weights(rate,Y_pred)\n",
    "        Y_pred = []\n",
    "        for x1,x2 in zip(X1,X2):\n",
    "            Y_pred.append(predict(x1,x2))\n",
    "        new_cost = calculate_cost(Y_pred)\n",
    "        count += 1\n",
    "    print \"Iterations : %d\" % count\n",
    "    print \"Last Cost : %lf\" % new_cost\n",
    "    print \"Second Last Cost : %f \" % current_cost \n",
    "    print \"Learning rate : %f \" % rate\n",
    "    print \"w0 : %f \" % w0\n",
    "    print \"w1 : %f\" % w1\n",
    "    print \"w2 : %f\" % w2\n",
    "    print \"RMSE : %f\" % sqrt(calculate_cost(Y_pred))\n",
    "    print \"\\n\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Best Result with Learning rate : 0.10000 \n",
    "\n",
    "Iterations : 1000 <br />\n",
    "Last Cost : 4089308608.459870 <br />\n",
    "Second Last Cost : 4089326941.073368 <br />\n",
    "Learning rate : 0.100000 <br />\n",
    "w0 : 196128.655623 <br />\n",
    "w1 : 497677.618939 <br />\n",
    "w2 : -24754.055071 <br />\n",
    "RMSE : 63947.702136 <br />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1a27a04910>]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY0AAAD8CAYAAACLrvgBAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvNQv5yAAAIABJREFUeJzt3Xl8VPX1//HXIQQIKgYUFVkEFde6gFNBrRZREcEW/dW22kWqtvRrrbW72PZbcWnLt/32615sXCpoFal1oSoiohS0gIRFEREJixJA9k3ZspzfH/MJTnAyc7PNTML7+XjMIzNnzr2fc28mczJ3G3N3REREomiR7QJERKTpUNMQEZHI1DRERCQyNQ0REYlMTUNERCJT0xARkcjSNg0zO9bM5iXctprZj82sg5lNMrPF4Wf7kG9mdreZlZjZ22bWO2FeQ0P+YjMbmhA/zczmh2nuNjML8aRjiIhIdqRtGu6+yN1PdfdTgdOA7cAzwHBgsrv3BCaHxwAXAT3DbRgwCuINALgZ6AOcDtyc0ARGhdyq6QaGeE1jiIhIFtR289R5wBJ3/wAYAowO8dHAJeH+EGCMx80ACs2sE3AhMMndN7r7JmASMDA8187dp3v8TMMxe80r2RgiIpIFLWuZfznwRLh/qLuvBnD31WZ2SIh3BlYkTFMaYqnipUniqcao0cEHH+zdu3evzTKJiOzzZs+evd7dO6bLi9w0zKwV8GXgpnSpSWJeh3hkZjaM+OYtunXrRnFxcW0mFxHZ55nZB1HyarN56iJgjruvCY/XhE1LhJ9rQ7wU6JowXRdgVZp4lyTxVGNU4+5F7h5z91jHjmkbpYiI1FFtmsYVfLppCmA8UHUE1FDguYT4leEoqr7AlrCJaSIwwMzahx3gA4CJ4bltZtY3HDV15V7zSjaGiIhkQaTNU2bWFrgA+H5CeCQwzsyuAT4EvhriLwKDgBLiR1pdBeDuG83sNmBWyLvV3TeG+9cCjwAFwIRwSzWGiIhkgTW3S6PHYjHXPg0Rkdoxs9nuHkuXpzPCRUQkMjUNERGJTE1DREQiU9MQEWninn97FUVTl5CJfdS1PSNcRERyRNHUJfz+xff2PP5W3yNo26px39bVNEREmpDKSufm8Qt4dManJ3C3MHjlp19s9IYBahoiIk3CzrIKvv/obP79/ro9se4HteWf157JQfu3zlgdahoiIjls4ye7uez+/7B03Sd7Ymf3PJiib8coaJWX8XrUNEREctAHGz7hgjumsru8ck/sitO7cvslJ5HXItl1XjNDTUNEJIfM/XATl/7lP9Viv7jwWH7Q7yjCl5pmlZqGiEgOeHnBRwx7dHa12J1fP5VLenWuYYrsUNMQEckSd2fM9A+4efyCavHHv9eHM486OEtVpaamISKSAbvKK7j2sTm8tmgtS343iD9MWMgD05ZVy3n5J+dwzKEHZKnCaNQ0REQa0fbd5XzrwZnM+XDzntiRv3pxz/1D27Vm/A+/wKHt2mSjvFpT0xARaQRbtpdx6ag3qh0qmyh2RHseufp09m/dtN6Gm1a1IiI5bu3WnQy6exrrP96d9PlLe3Xmj5edTH5e07z0n5qGiEgDWLFxO/3/PIWyiuQXDfxR/6P5yQXH5MRhs/WhpiEiUg+L12zjgjum1vj8yP93Epef3i2DFTUuNQ0RkTp4a8Vmhtz3Ro3P/+2qz3PusYdksKLMUNMQEamF6Us2cMUDM2p8/vnrv8DnOh+YwYoyS01DRCSCV95dw3fHFCd97sCCfF740Rfo0r5thqvKvEhNw8wKgQeBzwEOXA0sAp4EugPLga+5+yaL7+W5CxgEbAe+4+5zwnyGAr8Js73d3UeH+GnAI0AB8CJwg7u7mXVINkZ9FlhEpDaenbuSHz85L+lzn+vcjseu6UNh21YZrip7oh7zdRfwkrsfB5wCLASGA5PdvScwOTwGuAjoGW7DgFEAoQHcDPQBTgduNrP2YZpRIbdquoEhXtMYIiKNasz05XQf/kLShjHwxMN477aBPH/92ftUw4AInzTMrB1wDvAdAHffDew2syFAv5A2GpgC3AgMAcZ4/MtqZ5hZoZl1CrmT3H1jmO8kYKCZTQHaufv0EB8DXAJMCPNKNoaISINYsXE7LfOMTgcWAHDP5MX8edL7SXO/+4Ue3DTo+KxemjzbomyeOhJYB/zNzE4BZgM3AIe6+2oAd19tZlWHCXQGViRMXxpiqeKlSeKkGENEpF4Wrt7Kva+V8OL81Vx88uEcekBrHnx9WdLcm790At85s3uTP8eiIURpGi2B3sD17j7TzO4i9WaiZGvV6xCPzMyGEd+8Rbduzed4aBFpePNWbObeV0t4ZeEaWrdsgTv8661VSXPv/1ZvBn6uU4YrzG1RmkYpUOruM8Pjp4g3jTVm1il8AugErE3I75owfRdgVYj32ys+JcS7JMknxRjVuHsRUAQQi8Vq1XBEZN8wc+kG7n2thGmL19MqXMJjV8K34iX657VncNoRHTJZXpORtmm4+0dmtsLMjnX3RcB5wLvhNhQYGX4+FyYZD/zQzMYS3+m9JbzpTwR+n7DzewBwk7tvNLNtZtYXmAlcCdyTMK9kY4iIpOXuTF28nntfXcys5Zv27IvYXfHZZtGqZQteuuFsjuy4f6bLbFKinqdxPfB3M2sFLAWuIn7k1Tgzuwb4EPhqyH2R+OG2JcQPub0KIDSH24BZIe/Wqp3iwLV8esjthHCDeLNINoaISI0qK51XFq7h3tdKeLt0y554ReVnN0Qc2XE/nhx2Bh0PaJ3JEpssix/k1HzEYjEvLk5+Ao6ING8Vlc4L81dz36slLFqzLWVuv2M7cu83eje5S5M3FjOb7e6xdHlaWyLS5JVVVPLM3JWMmrKEZeuTf39FlW/17cZvLz6RVi2b5qXJs01NQ0SarJ1lFfxjdin3T1nCys07Uub+cuCxfP+co/bpcywagpqGiDQ523eX8/jMDymaupS123alzL3z66cy5NTDdY5FA1HTEJEmY+vOMsb8ZzkPvb6MTdvLUub+/bt9OOvogzNU2b5DTUNEct7GT3bz8OvLGD19Odt2lqfMnXDD2RzfqV1mCtsHqWmISM5au3UnD0xbymMzPmRHWUWNeZ0LCxj3X2fQubAgg9Xtm9Q0RCTnlG7azl//vZQni1ewu4aztgH69OjAX7992j53pdlsUtMQkZyxbP0njJpSwtNzVlKe5ES8Kl/p3YXbL/kcBa3yMlidgJqGiOSARR9t477XSnj+7VWk6BX8qP/RXH9eT/LzdI5FtqhpiEjWvF0av+Lsy++uSZn3+0tP4vLPd6WFzrHIOjUNEcm4Wcs3cu+rJfz7/XUp8x4aGqP/cYfoHIscoqYhIhnh7rxRsoF7Xl3MzGUbU+Y+84Mz6dWtfcocyQ41DRFpVO7O5IVruee1Et5asbnGvHZtWvLcD79Aj4P3y2B1UltqGiLSKCoqnQnvrOa+15awcPXWGvNO7nIgDw6NccgBbTJYndSVmoaINKiyikrGz1vFfVNKWLqu5ivODjrpMEZ+5WTatcnPYHVSX2oaItIgdpVX8NTsUkZNWULpppqvOPu9s3vwswHH0iZf51g0RWoaIlIvO3ZX8MSb8SvOfrR1Z415/33xCQw94wha6hyLJk1NQ0TqZNvOMh6d8QEPTVvGhk9215h3zxW9uPjkTjpstplQ0xCRWtm8fTcPv7GcR95YxtYUV5x9/Ht9OPMoXZq8uVHTEJFI1m3bxYOvL+Wx6R/wye6arzirS5M3b2oaIpLSqs07KJq6lCfe/JBdKa44O+2X59K1Q9sMVibZEKlpmNlyYBtQAZS7e8zMOgBPAt2B5cDX3H2TxTdc3gUMArYD33H3OWE+Q4HfhNne7u6jQ/w04BGgAHgRuMHdvaYx6rXEIhLJBxs+YdSUJfxzTillFcmvItjj4P146r/O4KD9W2e4OsmW2nzSONfd1yc8Hg5MdveRZjY8PL4RuAjoGW59gFFAn9AAbgZigAOzzWx8aAKjgGHADOJNYyAwIcUYItJIFq/Zxl+mLOG5eStrvOLseccdwl1X9GL/1tpYsa+pz298CNAv3B8NTCH+hj4EGOPuDswws0Iz6xRyJ7n7RgAzmwQMNLMpQDt3nx7iY4BLiDeNmsYQkQb2zsot3PdaCS8t+AivoVlcfVYPbrzoWFq31DkW+6qoTcOBl83Mgb+6exFwqLuvBnD31WZ2SMjtDKxImLY0xFLFS5PESTFGNWY2jPgnFbp16xZxkUQE4Kfj5vH0nJUpc3578QkMPbM7ebo0+T4vatM4y91XhTftSWb2XorcZK8qr0M8stDEigBisVitphXZF7k71z8xl+ffXp0y7y/f7M1FnztM51jIHpGahruvCj/XmtkzwOnAGjPrFD4BdALWhvRSoGvC5F2AVSHeb6/4lBDvkiSfFGOISB24O8Menc2kNF96NO77Z3B6jw4ZqkqakrTn85vZfmZ2QNV9YADwDjAeGBrShgLPhfvjgSstri+wJWximggMMLP2ZtY+zGdieG6bmfUNR15dude8ko0hIrVQWekMffhNetz0YsqGMekn57B85GA1DKlRlE8ahwLPhI+nLYHH3f0lM5sFjDOza4APga+G/BeJH25bQvyQ26sA3H2jmd0GzAp5t1btFAeu5dNDbieEG8DIGsYQkQjKKyr51kMzmbE09Zce/Wd4fw4vLMhQVdKUmdd0mEQTFYvFvLi4ONtliGTV7vJKvl40nbkf1vylRwBz//sC2u/XKkNVSS4zs9nuHkuXp4OsRZqRnWUVfGXUf1iwquYvPTqwIJ/pN/WnbSv9+Uvt6VUj0gx8vKucS+57g5K1H9eY06tbIU8OO4NWLXVpcqk7NQ2RJmzL9jIuvncaKzbW/KVH7dvmM/s3F9BC51hIA1DTEGmC1n+8i8F3T2PN1l015hx58H5M/tkXdY6FNCg1DZEm5KMtOxl411Q2by+rMedLpxzOPVf0ymBVsi9R0xBpAlZs3M6AO6ayo6zm77H4yfnHcMP5PTNYleyL1DREcljJ2o+58M6pVNR0uVngjq+fwqW9utT4vEhDUtMQyUHvrtrKoLunpczR16lKNqhpiOSQuR9u4tK//Cdlzss/OYdjDj0gQxWJVKemIZJl7s7MZRu5vGhGyrwZN53HYQe2yVBVIsmpaYhkibvz7/fX8Z2/zUqZ9/aIAbRrk5+hqkRSU9MQybDKSmfSwjV8/9HZKfMW3T5Q35AnOUdNQyRDKiqd599exQ1j56XMW/L7QfqGPMlZahoijaysopJn5q7kl0+9nTJv2R8G6extyXlqGiKNZM3WnZzxh8mkOMWCNvkteO+2izJXlEg9qWmINLBFH23jwjunpsw58fB2vPCjszNUkUjDUdMQaSCvL17Ptx6amTJn8EmduO+bvTNUkUjDU9MQqadxxSvS7q8Yds6R/GrQ8RmqSKTxqGmI1EFFpXPnK+9zz6slKfN+M/h4vnv2kRmqSqTxqWmI1MLOsgr6/+8UVm3ZmTLv7it68eVTDs9QVSKZE7lpmFkeUAysdPeLzawHMBboAMwBvu3uu82sNTAGOA3YAHzd3ZeHedwEXANUAD9y94khPhC4C8gDHnT3kSGedIx6L7VILW38ZDe9b5uUNk8XEZTmrjZfFnwDsDDh8f8Ad7h7T2AT8WZA+LnJ3Y8G7gh5mNkJwOXAicBA4C9mlhea0X3ARcAJwBUhN9UYIhnxwYZP6D78hbQNY+ywviwfOVgNQ5q9SE3DzLoAg4EHw2MD+gNPhZTRwCXh/pDwmPD8eSF/CDDW3Xe5+zKgBDg93ErcfWn4FDEWGJJmDJFGNW/FZroPf4Ev/mlKyrwpP+/H8pGD6XvkQZkpTCTLom6euhP4JVB1PeaDgM3uXh4elwKdw/3OwAoAdy83sy0hvzOQeBnPxGlW7BXvk2YMkQZXWem8+t5avjumOG3u7N+cz0H7t85AVSK5JW3TMLOLgbXuPtvM+lWFk6R6mudqiif7tJMqP1mNw4BhAN26dUuWIlKjnWUVPD1nJb96Zn7a3AW3XMh+rXX8iOy7orz6zwK+bGaDgDZAO+KfPArNrGX4JNAFWBXyS4GuQKmZtQQOBDYmxKskTpMsvj7FGNW4exFQBBCLxVJctEHkU1u2l/HAtKXc+1rqw2ZBV5wVqZK2abj7TcBNAOGTxs/d/Ztm9g/gMuL7IIYCz4VJxofH08Pzr7q7m9l44HEz+z/gcKAn8CbxTxQ9w5FSK4nvLP9GmOa1GsYQqbMVG7czcsJ7vDB/ddpcXXFWpLr6fM6+ERhrZrcDc4GHQvwh4FEzKyH+CeNyAHdfYGbjgHeBcuA6d68AMLMfAhOJH3L7sLsvSDOGSK3NL93CT8fNY/Haj1PmtW+bz5z/vkBXnBVJwtyb19acWCzmxcXpd2TKvsHdmfL+Oq5K8+14ALEj2vPUtWdmoCqR3GNms909li5Pe/SkWdpdXslz81byizTXhAL4Su8u/Plrp2SgKpGmT01DmpUtO8r4+8wP+ONLi9LmXt//aH424NgMVCXSfKhpSLOwavMO/vrvJYye/kHa3F8NOo5h5xyVgapEmh81DWnSFqzawh2T3ueVhWvT5uqKsyL1p6YhTY67M23xekb8awFL132SNv++b/Rm8MmdMlCZSPOnpiFNRllFJc+/vYqfPPlWpPyxw/rqmlAiDUxNQ3Letp1ljH1zBb97cWH6ZOClH5/NcYe1a+SqRPZNahqSsz7aspOH31hG0dSlkfKn/fJcunZo28hViezb1DQk57z30VbumVwS6TIfoCvOimSSmobkBHdn+pINjPjXAt5fk/oyH1UW3jqQgla6iKBIJqlpSFaVV1TywvzV3DB2XuRpSn53ES3zavOlkyLSUNQ0JCs+2VXOk7NWcOvz70aeZunvB9FCV5wVySo1DcmotVt38tAby/jrv6Pt3O5cWMDrN56rK86K5Ag1DcmIkrXb+PPL7zPhnY8i5X/h6IN57Lt9GrkqEaktNQ1pNO7Oz8a9xdNzV0ae5rpzj+IXFx7XiFWJSH2oaUiDq6h0Rk0p4X9ffj/yNH/8ysl87fNd0yeKSFapaUiD2bG7gr9OXcKdryyOPM0T3+vLGUfpUh8iTYWahtTbho93cffkxZEuS17luevO4pSuhY1YlYg0BjUNqbNl6z/htuff5dX30l+WvMq8315AYdtWjViViDQmNQ2ptTkfbuK6v89h9ZadkfLb5Ldg3m8H0CZfZ2+LNHVqGhJJZaUzaeEavv/o7FpN995tA9UsRJqRtNdiMLM2Zvammb1lZgvM7JYQ72FmM81ssZk9aWatQrx1eFwSnu+eMK+bQnyRmV2YEB8YYiVmNjwhnnQMyZydZRU8/PoyjvzVi7VqGLN+fT7LRw5WwxBpZqJ80tgF9Hf3j80sH3jdzCYAPwXucPexZnY/cA0wKvzc5O5Hm9nlwP8AXzezE4DLgROBw4FXzOyYMMZ9wAVAKTDLzMa7+7th2mRjSCPbsr2M/315EY/OiL5z+4DWLXnz1+frIoIizVjapuHuDlRddjQ/3BzoD3wjxEcDI4i/oQ8J9wGeAu61+DUghgBj3X0XsMzMSoDTQ16Juy8FMLOxwBAzW5hiDGkkpZu289Dry/jbG8sjTzP0jCMY8eUTdakPkX1ApH0aZpYHzAaOJv6pYAmw2d3LQ0op0Dnc7wysAHD3cjPbAhwU4jMSZps4zYq94n3CNDWNsXd9w4BhAN26dYuySLKXd1ZuoWjqUsa/tSryNKOvPp0vHtOxEasSkVwTqWm4ewVwqpkVAs8AxydLCz+T/bvpKeLJ9qukyk9WXxFQBBCLxZLmyGe5O1MXr2fUlBJmLN0Yebo3f30ehxzQphErE5FcVaujp9x9s5lNAfoChWbWMnwS6AJU/YtaCnQFSs2sJXAgsDEhXiVxmmTx9SnGkHooq6jkX2+t4s5XFvPhxu2RpundrZBx3z9D32Mhso9L2zTMrCNQFhpGAXA+8R3UrwGXAWOBocBzYZLx4fH08Pyr7u5mNh543Mz+j/iO8J7Am8Q/UfQ0sx7ASuI7y78RpqlpDKmDbTvLGPvmCn734sLI0/z24hO46qzu2l8hIkC0TxqdgNFhv0YLYJy7P29m7wJjzex2YC7wUMh/CHg07OjeSLwJ4O4LzGwc8C5QDlwXNnthZj8EJgJ5wMPuviDM68YaxpBaWLN1Jw/X4jssAMb/8CxO7qLLfIhIdRY/OKr5iMViXlxcnO0ycsL7a7ZRNHUpT80ujTzN7N+cz0H7t27EqkQkF5nZbHePpcvTGeHNjLszY+lGiqYu4bVF6yJNc95xh/CXb/WmdUudXyEiqalpNBPlFZW8tOAj7nttCQtXb400ze8vPYkrTu+q/RUiEpmaRhO3Y3cF/5i9gj9NXMS2neXpJwCeve4sTtVlyUWkDtQ0mqgNH+9i9PQPeHT6cjZtL4s0zYybzuOwA3V+hYjUnZpGE7Ns/Sc8OG0p/5hdyu7yyrT5HQ9ozZSf92O/1vpVi0j96Z2kiZjz4SaK/r2UlxZ8FCn/6rN68OvBx5PXQvsrRKThqGnksMpKZ/J7aymauoRZyzdFmqbo26cx4MTDGrkyEdlXqWnkoJ1lFTw7dyUPTFvKknWfRJrmlZ9+kaMP2b+RKxORfZ2aRg7Zsr2Mx2Z+wN/eWM76j3dFmkbfuS0imaSmkQOqvsPiyVkr2L67ItI0i24fqJPxRCTj1DSyqOo7LF6Yv5qKyvjlXNq1acnWGs636NWtkKevPVMn44lI1qhpZFjVd1gUTV3CGyUb9sRP6VrIxzvLku7DuHHgcVzb76hMlikikpSaRoZUfYdF0dSlvPfRNgBatWzBBccfyq7yCl5ZuPYz0zz1X2cQ694h06WKiNRITaORVX2HxcNvLGP1lp0AdC4s4PzjD+Gt0i28MH/1Z6bRlWZFJFepaTSSqu+weHzmh3uuCXV2z4M5tWshj834gNHTP/jMNCW/u0jfjCciOU1No4FVfYfFc/NWUlbhHNC6JVeecQQH7deaO155n2mL11fL71xYwBvD+2epWhGR2lHTaADJvsPi2EMPYEivw1mzZWfSTxU/u+AYrj+vZ6ZLFRGpFzWNeqj6DosHpi7lrdIt5LUwBp/UiXOOOZixs1bwx5cWfWaaO79+Kpf06pyFakVE6k9No442fLyLy+6fzrL1n3Dw/q35Uf+jObywgOFPz0+6c/uRqz5Pv2MPyUKlIiINR02jjlrmteDz3dvzg35H8cGG7dz9akm156879ygGn3Q4jnPi4QdmqUoRkYaVtmmYWVdgDHAYUAkUuftdZtYBeBLoDiwHvubumyx+uvJdwCBgO/Add58T5jUU+E2Y9e3uPjrETwMeAQqAF4Eb3N1rGqPeS90AdpdXsnzDdsYVl1aLP3hljP7HHUILXZJcRJqhKJ80yoGfufscMzsAmG1mk4DvAJPdfaSZDQeGAzcCFwE9w60PMAroExrAzUAM8DCf8aEJjAKGATOIN42BwIQwz2RjZE3x8o1cdv/0arFTuhzIvd/oTdcObbNUlYhIZqRtGu6+Glgd7m8zs4VAZ2AI0C+kjQamEH9DHwKMcXcHZphZoZl1CrmT3H0jQGg8A81sCtDO3aeH+BjgEuJNo6YxMqqy0nlg2lL+MOG9avHrzj2K6/v3pE2+LhwoIvuGWu3TMLPuQC9gJnBoaCi4+2ozq9rL2xlYkTBZaYilipcmiZNijIzYurOMax+bXe0aUQCjrz6dLx7TMZOliIjkhMhNw8z2B/4J/Njdt6a40mqyJ7wO8cjMbBjxzVt069atNpMmNb90C1+69/VqseM7teORqz7Poe3a1Hv+IiJNVaSmYWb5xBvG39396RBeY2adwieATkDVFfdKga4Jk3cBVoV4v73iU0K8S5L8VGNU4+5FQBFALBarVcNJmAd/e2M5tz7/brX4f33xKH4+4Bhd3kNEBEj7ThiOhnoIWOju/5fw1HhgaLg/FHguIX6lxfUFtoRNTBOBAWbW3szaAwOAieG5bWbWN4x15V7zSjZGg7vlX+9Waxhjrj6d5SMHM/yi49QwRESCKJ80zgK+Dcw3s3kh9itgJDDOzK4BPgS+Gp57kfjhtiXED7m9CsDdN5rZbcCskHdr1U5x4Fo+PeR2QriRYowGd2mvzqzbtoubv3QCh2gTlIhIUhY/yKn5iMViXlxcnO0yRESaFDOb7e6xdHna7iIiIpGpaYiISGRqGiIiEpmahoiIRKamISIikalpiIhIZGoaIiISmZqGiIhEpqYhIiKRqWmIiEhkahoiIhKZmoaIiESmpiEiIpGpaYiISGRqGiIiEpmahoiIRKamISIikalpiIhIZGoaIiISmZqGiIhEpqYhIiKRpW0aZvawma01s3cSYh3MbJKZLQ4/24e4mdndZlZiZm+bWe+EaYaG/MVmNjQhfpqZzQ/T3G1mlmoMERHJniifNB4BBu4VGw5MdveewOTwGOAioGe4DQNGQbwBADcDfYDTgZsTmsCokFs13cA0Y4iISJakbRruPhXYuFd4CDA63B8NXJIQH+NxM4BCM+sEXAhMcveN7r4JmAQMDM+1c/fp7u7AmL3mlWwMERHJkrru0zjU3VcDhJ+HhHhnYEVCXmmIpYqXJomnGuMzzGyYmRWbWfG6devquEgiIpJOQ+8ItyQxr0O8Vty9yN1j7h7r2LFjbScXEZGI6to01oRNS4Sfa0O8FOiakNcFWJUm3iVJPNUYIiKSJXVtGuOBqiOghgLPJcSvDEdR9QW2hE1LE4EBZtY+7AAfAEwMz20zs77hqKkr95pXsjFERCRLWqZLMLMngH7AwWZWSvwoqJHAODO7BvgQ+GpIfxEYBJQA24GrANx9o5ndBswKebe6e9XO9WuJH6FVAEwIN1KMISIiWWLxg5aaj1gs5sXFxdkuQ0SkSTGz2e4eS5enM8JFRCQyNQ0REYlMTUNERCJT0xARkcjUNEREJDI1DRERiUxNQ0REIlPTEBGRyNQ0REQkMjUNERGJTE1DREQiU9MQEZHI1DRERCQyNQ0REYlMTUNERCJT0xARkcjUNEREJDI1DRERiUxNQ0REIlPTEBGRyHK+aZjZQDNbZGYlZjY82/WIiOzLWmZCF77AAAAJHklEQVS7gFTMLA+4D7gAKAVmmdl4d3+3Icd5du5K/jRxEas27+DwwgJ+ceGxXNKrc73y0+U8O3clI8YvYPOOsviyAr5nucEdOhcWcO5xHXntvXXV5gPsmXdh23x2llWwo6xyz7xbGFQ6tG+bjzts2VFGYcL9xHp+8+x8npi5ggp38sy4ok9XYkd0qPXyVdW5cvOOasvSumULdpdX4kCeGX2PbM/yDTtYuXkHeWZUuO9ZXojXPPjkTjUuc+J0hQX5mMHm7dWX78CEeGL9UX4nUV8Hibl7j5e4Lqpq7RzhddVQUtVWl9dqc5bJZW+ssTL9+zOv+mvNQWZ2BjDC3S8Mj28CcPc/1DRNLBbz4uLiyGM8O3clNz09nx1lFXtiBfl5/OH/nZR0xUfJT5fz7NyV/OIfb1FWWft1n59n4NRp2kQF+Xn07nYgbyzZ+Jnn8loYFQnzj7J8jam+y1yQn8dXTuvMP2evTPk7ifo6qOvyp3pdNZR0tdX2tdqcZXLZG2ushpyvmc1291i6vFzfPNUZWJHwuDTEGsyfJi76zB/YjrIK/jRxUZ3z0+X8aeKiOr8BllV4vRtGVT3JGgZQrWFU5aZbvsZU32XeUVbBEzNXpP2dRH0d1HX5U72uGkq62mr7Wm3OMrnsjTVWNn5/ud40LEnsM+8eZjbMzIrNrHjdunW1GmDV5h0NHk+XU9PzuSzK8uWyiho+Uaf7nSSL12f5G3vdRZl/bV6rzVkml72xxsrG7y/Xm0Yp0DXhcRdg1d5J7l7k7jF3j3Xs2LFWAxxeWNDg8XQ5NT2fy6IsXy7Ls2T/f6T/nSSL12f5G3vdRZl/bV6rzVkml72xxsrG7y/Xm8YsoKeZ9TCzVsDlwPiGHOAXFx5LQX5etVhBft6ena91yU+X84sLjyW/RfI3sXTy86zO0+5dz1lHdUj6XN5e84+yfI2pvstckJ/HFX26pv2dRH0d1HX5U72uGkq62mr7Wm3OMrnsjTVWNn5/eSNGjGi0mdfXiBEjKm+55ZbFwN+B64HH3P2fqaYpKioaMWzYsMhjHNepHV3aFzB/5RY+3llO58ICfvulE2rciRQlP13OcZ3a0a1DW2Ys3cDO8vhRT4lviVX/FHcuLGDIqYez4ePde+Yz4ssnMuDEw/bMu33bfAwoT9jm3yK+35j2bfNp0zKPXeWV1e5X1fPrwSew/uNdLFi5dc/RTd/s242rzuxR6+WrqnPbzvJqy9K6ZQsqQ215Zpx5VAcqHbbtLCfPDE9YXkLNl53WpcZlTpyusCCfglZ57CqrvnyJ8ar6f3Du0Wl/J1FfB3vn7j1e4rqoqjXd66qhpKuttq/V5iyTy95YYzXkfG+55ZbVI0aMKEqXl9NHT9VFbY+eEhGR5nP0lIiI5BA1DRERiUxNQ0REIlPTEBGRyNQ0REQksmZ39JSZrQM+yNBwBwPrMzRWfajOhqU6G5bqbFh1rfMId097dnSzaxqZZGbFUQ5RyzbV2bBUZ8NSnQ2rsevU5ikREYlMTUNERCJT06iftKfc5wjV2bBUZ8NSnQ2rUevUPg0REYlMnzRERCQyNY29mNnDZrbWzN5JiHUws0lmtjj8bB/iZmZ3m1mJmb1tZr0Tphka8heb2dAM1TnCzFaa2bxwG5Tw3E2hzkVmdmFCfGCIlZjZ8Eaos6uZvWZmC81sgZndEOI5tU5T1JlT69TM2pjZm2b2VqjzlhDvYWYzw7p5MnyVAGbWOjwuCc93T1d/I9b4iJktS1iXp4Z41v6Owhh5ZjbXzJ4Pj3NmXaapMzvr0911S7gB5wC9gXcSYn8Ehof7w4H/CfcHAROIX9m8LzAzxDsAS8PP9uF++wzUOQL4eZLcE4C3gNZAD2AJkBduS4AjgVYh54QGrrMT0DvcPwB4P9STU+s0RZ05tU7Detk/3M8HZob1NA64PMTvB64N938A3B/uXw48mar+Rq7xEeCyJPlZ+zsK4/wUeBx4PjzOmXWZps6srE990tiLu08F9v7y7CHA6HB/NHBJQnyMx80ACs2sE3AhMMndN7r7JmASMDADddZkCDDW3Xe5+zKgBDg93Ercfam77wbGhtyGrHO1u88J97cBC4l/z3tOrdMUddYkK+s0rJePw8P8cHOgP/BUiO+9PqvW81PAeWZmKepvzBprkrW/IzPrAgwGHgyPjRxalzXVmUajrk81jWgOdffVEH9zAQ4J8c7AioS80hCrKZ4JPwwfSR+u2uSTop6M1hk+zvci/p9nzq7TveqEHFunYTPFPGAt8T/8JcBmdy9PMuaeesLzW4CDGrvOvWt096p1+buwLu8ws9Z717hXLZn4nd8J/BKoDI8PIsfWZQ11Vsn4+lTTqJ9k30HqKeKNbRRwFHAqsBr4c4hnvU4z2x/4J/Bjd9+aKrWGmjJSa5I6c26dunuFu58KdCH+H+3xKcbMSp1712hmnwNuAo4DPk98E8mN2azRzC4G1rr77MRwijFzqU7I0vpU04hmTfh4R/i5NsRLga4JeV2AVSnijcrd14Q/1krgAT79iJzVOs0sn/gb8d/d/ekQzrl1mqzOXF2nobbNwBTi260LzaxlkjH31BOeP5D4Zs2M1JlQ48CwCdDdfRfwN7K/Ls8Cvmxmy4lvRuxP/D/6XFuXn6nTzB7L2vqsyw6Z5n4DulN9B/OfqL7T9o/h/mCq73B60z/d4bSM+M6m9uF+hwzU2Snh/k+Ib2cFOJHqO+qWEt9h2zLc78GnO21PbOAaDRgD3LlXPKfWaYo6c2qdAh2BwnC/AJgGXAz8g+o7b38Q7l9H9Z2341LV38g1dkpY13cCI3Ph7yiM1Y9PdzDnzLpMU2dW1meDL1RTvwFPEN8MUUa8M19DfLvlZGBx+Nkh4Zd1H/FtyvOBWMJ8ria+Q6wEuCpDdT4a6ngbGE/1N7xfhzoXARclxAcRP1JoCfDrRqjzC8Q/Ar8NzAu3Qbm2TlPUmVPrFDgZmBvqeQf4bYgfCbwZ1s0/gNYh3iY8LgnPH5mu/kas8dWwLt8BHuPTI6yy9neUME4/Pn0zzpl1mabOrKxPnREuIiKRaZ+GiIhEpqYhIiKRqWmIiEhkahoiIhKZmoaIiESmpiEiIpGpaYiISGRqGiIiEtn/Bwj5vLa6yRBrAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a27a04610>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(data[0],data[1])\n",
    "plt.plot(data[0],Y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2-1. Linear regression with two variables using matrix\n",
    "Fit the data (ex1data2.csv) using matrix calculation. You need to normalize variables. You need to calculate\n",
    "cost function and update weight.\n",
    "Please print the Root Mean Squared Error (RMSE) after optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "X1 = data[0]\n",
    "X2 = data[1]\n",
    "Y = data[2]\n",
    "m = len(data)\n",
    "\n",
    "X1 = X1.astype(float) \n",
    "X2 = X2.astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "X1_min = X1.min()\n",
    "X1_max = X1.max()\n",
    "X2_min = X2.min()\n",
    "X2_max = X2.max()\n",
    "i = 0\n",
    "for x1,x2 in zip(X1,X2):\n",
    "    X1[i] = ((x1 - X1_min)/(X1_max - X1_min))\n",
    "    X2[i] = ((x2 - X2_min)/(X2_max - X2_min))\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = pd.Series( (1 for i in range(0,len(data))) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_mat = np.asmatrix((np.column_stack((temp,X1,X2))))\n",
    "W_mat = np.asmatrix(np.array([0., 0., 0.])).T\n",
    "Y_mat = np.asmatrix((np.row_stack((Y))))\n",
    "learning_rates = [0.1,0.01,0.001,0.0001,0.00001,0.000001,0.0000001,0.00000000001]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(47, 3)\n",
      "(47, 1)\n",
      "(3, 1)\n"
     ]
    }
   ],
   "source": [
    "print(X_mat).shape\n",
    "print(Y_mat).shape\n",
    "print(W_mat).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_cost():\n",
    "    result =  np.dot((np.matmul(X_mat,W_mat) - Y_mat).T,(np.matmul(X_mat,W_mat) - Y_mat))\n",
    "    result /=m\n",
    "    return float(result)\n",
    "\n",
    "def cost_derivative():\n",
    "    result = (np.matmul(np.matmul(X_mat.T,X_mat),W_mat)-(np.matmul(X_mat.T,Y_mat)))\n",
    "    result *= 2\n",
    "    result /=m\n",
    "    return result\n",
    "\n",
    "def calculate_weights(rate):\n",
    "    global W_mat\n",
    "    W_mat = W_mat - rate*cost_derivative()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iterations : 1000\n",
      "Last Cost : 4089308608.459871\n",
      "Second Last Cost : 4089326941.073368 \n",
      "Learning rate : 0.100000 \n",
      "w0 : 196128.655623 \n",
      "w1 : 497677.618939\n",
      "w2 : -24754.055071\n",
      "RMSE : 63947.702136\n",
      "\n",
      "\n",
      "Iterations : 1000\n",
      "Last Cost : 5748738122.622198\n",
      "Second Last Cost : 5750576554.169697 \n",
      "Learning rate : 0.010000 \n",
      "w0 : 184853.786354 \n",
      "w1 : 278567.520649\n",
      "w2 : 126710.056154\n",
      "RMSE : 75820.433411\n",
      "\n",
      "\n",
      "Iterations : 1000\n",
      "Last Cost : 9796267918.101746\n",
      "Second Last Cost : 9799444614.727167 \n",
      "Learning rate : 0.001000 \n",
      "w0 : 220503.260541 \n",
      "w1 : 105019.575118\n",
      "w2 : 130537.263266\n",
      "RMSE : 98976.097711\n",
      "\n",
      "\n",
      "Iterations : 1000\n",
      "Last Cost : 78893814028.973938\n",
      "Second Last Cost : 78932753412.426193 \n",
      "Learning rate : 0.000100 \n",
      "w0 : 59194.384597 \n",
      "w1 : 23144.181378\n",
      "w2 : 33994.496394\n",
      "RMSE : 280880.426568\n",
      "\n",
      "\n",
      "Iterations : 1000\n",
      "Last Cost : 124532273309.633743\n",
      "Second Last Cost : 124538738613.938934 \n",
      "Learning rate : 0.000010 \n",
      "w0 : 6711.735791 \n",
      "w1 : 2582.754669\n",
      "w2 : 3845.450319\n",
      "RMSE : 352891.305234\n",
      "\n",
      "\n",
      "Iterations : 1000\n",
      "Last Cost : 130501084928.845078\n",
      "Second Last Cost : 130501765095.757126 \n",
      "Learning rate : 0.000001 \n",
      "w0 : 679.852159 \n",
      "w1 : 261.213596\n",
      "w2 : 389.429800\n",
      "RMSE : 361249.339001\n",
      "\n",
      "\n",
      "Iterations : 1000\n",
      "Last Cost : 131114789685.871964\n",
      "Second Last Cost : 131114858048.413910 \n",
      "Learning rate : 0.000000 \n",
      "w0 : 68.072811 \n",
      "w1 : 26.151014\n",
      "w2 : 38.992283\n",
      "RMSE : 362097.762608\n",
      "\n",
      "\n",
      "Iterations : 1000\n",
      "Last Cost : 131183164649.254089\n",
      "Second Last Cost : 131183164656.094162 \n",
      "Learning rate : 0.000000 \n",
      "w0 : 0.006808 \n",
      "w1 : 0.002615\n",
      "w2 : 0.003900\n",
      "RMSE : 362192.165362\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for rate  in learning_rates:\n",
    "    W_mat = np.asmatrix(np.array([0., 0., 0.])).T\n",
    "    count = 0\n",
    "    max_count = 1000\n",
    "    current_cost = calculate_cost()\n",
    "    new_cost = 0\n",
    "    while new_cost < current_cost and count < max_count:\n",
    "        current_cost = calculate_cost()\n",
    "        calculate_weights(rate)\n",
    "        new_cost = calculate_cost()\n",
    "        count += 1\n",
    "    print \"Iterations : %d\" % count\n",
    "    print \"Last Cost : %lf\" % new_cost\n",
    "    print \"Second Last Cost : %f \" % current_cost \n",
    "    print \"Learning rate : %f \" % rate\n",
    "    print \"w0 : %f \" % W_mat[0]\n",
    "    print \"w1 : %f\" % W_mat[1]\n",
    "    print \"w2 : %f\" % W_mat[2]\n",
    "    print \"RMSE : %f\" % sqrt(calculate_cost())\n",
    "    print \"\\n\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Best Result with Learning rate : 0.10000 \n",
    "\n",
    "Iterations : 1000 <br/>\n",
    "Last Cost : 4089308608.459870 <br/>\n",
    "Second Last Cost : 4089326941.073366 <br/> \n",
    "Learning rate : 0.100000 <br/>\n",
    "w0 : 196128.655623 <br/>\n",
    "w1 : 497677.618939<br/>\n",
    "w2 : -24754.055071<br/>\n",
    "RMSE : 63947.702136<br/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2-2. Linear regression with two variables using Normal equation\n",
    "Fit the data (ex1data2.csv) using Normal equation. You need to calculate cost function and update weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "X1 = data[0]\n",
    "X2 = data[1]\n",
    "Y = data[2]\n",
    "m = len(data)\n",
    "\n",
    "X1 = X1.astype(float) \n",
    "X2 = X2.astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "X1_min = X1.min()\n",
    "X1_max = X1.max()\n",
    "X2_min = X2.min()\n",
    "X2_max = X2.max()\n",
    "i = 0\n",
    "for x1,x2 in zip(X1,X2):\n",
    "    X1[i] = ((x1 - X1_min)/(X1_max - X1_min))\n",
    "    X2[i] = ((x2 - X2_min)/(X2_max - X2_min))\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = pd.Series( (1 for i in range(0,len(data))) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_mat = np.asmatrix((np.column_stack((temp,X1,X2))))\n",
    "W_mat = np.asmatrix(np.array([0., 0., 0.])).T\n",
    "Y_mat = np.asmatrix((np.row_stack((Y))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(47, 3)\n",
      "(47, 1)\n",
      "(3, 1)\n"
     ]
    }
   ],
   "source": [
    "print(X_mat).shape\n",
    "print(Y_mat).shape\n",
    "print(W_mat).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weight_function():\n",
    "    global Y_mat\n",
    "    result = np.matmul(np.matmul(np.matmul(X_mat.T,X_mat).I,X_mat.T),Y_mat)\n",
    "    \n",
    "    return result\n",
    "def predict_Y(theta):\n",
    "    result = np.matmul(X_mat,theta)\n",
    "    return result\n",
    "def calculate_cost(Y_pred):\n",
    "    result = 0\n",
    "    for i in range(0,len(Y_pred)):\n",
    "        result += (Y_pred[i] - Y_mat[i])**2\n",
    "    result /=m\n",
    "    return result   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta = weight_function()\n",
    "Y_pred = predict_Y(theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w0 : 199467.311263 \n",
      "w1 : 504777.761242\n",
      "w2 : -34951.661681\n",
      "RMSE : 63926.214926\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print \"w0 : %f \" % theta[0]\n",
    "print \"w1 : %f\" % theta[1]\n",
    "print \"w2 : %f\" % theta[2]\n",
    "print \"RMSE : %f\" % sqrt(calculate_cost(Y_pred))\n",
    "print \"\\n\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Result for Normal equation\n",
    "\n",
    "w0 : 199467.311263 <br />\n",
    "w1 : 504777.761242 <br />\n",
    "w2 : -34951.661681 <br />\n",
    "RMSE : 63926.214926 <br />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3(60pts). Linear regression with multiple variables\n",
    "Using Jupyter notebook, load the data (ex1data3.csv).\n",
    "This is California housing dataset. The original database is available from http://lib.stat.cmu.edu\n",
    "The data contains 20,640 observations on 9 variables. This dataset contains the average house value as target\n",
    "variable and the following input variables (features): average income, housing average age, average rooms,\n",
    "average bedrooms, population, average occupation, latitude, and longitude\n",
    "(R. Kelley and Ronald Barry, Sparse Spatial Autoregressions,\\nStatistics and Probability Letters, 33 (1997)\n",
    "291-297) ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3-1. Linear regression with multiple variables using matrix\n",
    "Fit the data (ex1data3.csv) using matrix calculation. You need to calculate cost function and update weight.\n",
    "You need to normalize variables. Please print the Root Mean Squared Error (RMSE) after optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('./ex1data3.csv', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "X1 = data[0]\n",
    "X2 = data[1]\n",
    "X3 = data[2]\n",
    "X4 = data[3]\n",
    "X5 = data[4]\n",
    "X6 = data[5]\n",
    "X7 = data[6]\n",
    "X8 = data[7]\n",
    "Y = data[8]\n",
    "m = len(data)\n",
    "\n",
    "X1 = X1.astype(float) \n",
    "X2 = X2.astype(float)\n",
    "X3 = X3.astype(float) \n",
    "X4 = X4.astype(float)\n",
    "X5 = X5.astype(float) \n",
    "X6 = X6.astype(float)\n",
    "X7 = X7.astype(float) \n",
    "X8 = X8.astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "X1_min = X1.min()\n",
    "X2_min = X2.min()\n",
    "X3_min = X3.min()\n",
    "X4_min = X4.min()\n",
    "X5_min = X5.min()\n",
    "X6_min = X6.min()\n",
    "X7_min = X7.min()\n",
    "X8_min = X8.min()\n",
    "X1_max = X1.max()\n",
    "X2_max = X2.max()\n",
    "X3_max = X3.max()\n",
    "X4_max = X4.max()\n",
    "X5_max = X5.max()\n",
    "X6_max = X6.max()\n",
    "X7_max = X7.max()\n",
    "X8_max = X8.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "for x1,x2,x3,x4,x5,x6,x7,x8 in zip(X1,X2,X3,X4,X5,X6,X7,X8):\n",
    "    X1[i] = ((x1 - X1_min)/(X1_max - X1_min))\n",
    "    X2[i] = ((x2 - X2_min)/(X2_max - X2_min))\n",
    "    X3[i] = ((x3 - X3_min)/(X3_max - X3_min))\n",
    "    X4[i] = ((x4 - X4_min)/(X4_max - X4_min))\n",
    "    X5[i] = ((x5 - X5_min)/(X5_max - X5_min))\n",
    "    X6[i] = ((x6 - X6_min)/(X6_max - X6_min))\n",
    "    X7[i] = ((x7 - X7_min)/(X7_max - X7_min))\n",
    "    X8[i] = ((x8 - X8_min)/(X8_max - X8_min))\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = pd.Series( (1 for i in range(0,len(data))) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_mat = np.asmatrix((np.column_stack((temp,X1,X2,X3,X4,X5,X6,X7,X8))))\n",
    "W_mat = np.asmatrix(np.array([0., 0., 0., 0., 0., 0., 0., 0., 0.])).T\n",
    "Y_mat = np.asmatrix((np.row_stack((Y))))\n",
    "learning_rates = [0.1,0.01,0.001,0.0001,0.00001,0.000001,0.0000001,0.00000000001]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_cost():\n",
    "    result =  np.dot((np.matmul(X_mat,W_mat) - Y_mat).T,(np.matmul(X_mat,W_mat) - Y_mat))\n",
    "    result /=m\n",
    "    return float(result)\n",
    "\n",
    "def cost_derivative():\n",
    "    result = (np.matmul(np.matmul(X_mat.T,X_mat),W_mat)-(np.matmul(X_mat.T,Y_mat)))\n",
    "    result *= 2\n",
    "    result /=m\n",
    "    return result\n",
    "\n",
    "def calculate_weights(rate):\n",
    "    global W_mat\n",
    "    W_mat = W_mat - rate*cost_derivative()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iterations : 1000\n",
      "Last Cost : 0.579215\n",
      "Second Last Cost : 0.579251 \n",
      "Learning rate : 0.100000 \n",
      "w0 : 1.703980 \n",
      "w1 : 5.861136\n",
      "w2 : 0.776703\n",
      "w3 : 0.154956 \n",
      "w4 : 0.072752\n",
      "w5 : 0.142323\n",
      "w6 : -0.063152 \n",
      "w7 : -1.788810\n",
      "w8 : -1.780212\n",
      "RMSE : 0.761062\n",
      "\n",
      "\n",
      "Iterations : 1000\n",
      "Last Cost : 0.959612\n",
      "Second Last Cost : 0.959848 \n",
      "Learning rate : 0.010000 \n",
      "w0 : 1.191202 \n",
      "w1 : 2.005130\n",
      "w2 : 0.626023\n",
      "w3 : 0.089956 \n",
      "w4 : 0.016753\n",
      "w5 : 0.053652\n",
      "w6 : -0.003183 \n",
      "w7 : -0.103988\n",
      "w8 : 0.213868\n",
      "RMSE : 0.979598\n",
      "\n",
      "\n",
      "Iterations : 1000\n",
      "Last Cost : 1.258245\n",
      "Second Last Cost : 1.258322 \n",
      "Learning rate : 0.001000 \n",
      "w0 : 1.176971 \n",
      "w1 : 0.476372\n",
      "w2 : 0.645939\n",
      "w3 : 0.044408 \n",
      "w4 : 0.025288\n",
      "w5 : 0.047160\n",
      "w6 : 0.001749 \n",
      "w7 : 0.319798\n",
      "w8 : 0.533617\n",
      "RMSE : 1.121715\n",
      "\n",
      "\n",
      "Iterations : 1000\n",
      "Last Cost : 3.482436\n",
      "Second Last Cost : 3.483925 \n",
      "Learning rate : 0.000100 \n",
      "w0 : 0.350871 \n",
      "w1 : 0.102299\n",
      "w2 : 0.195047\n",
      "w3 : 0.012016 \n",
      "w4 : 0.007792\n",
      "w5 : 0.013852\n",
      "w6 : 0.000625 \n",
      "w7 : 0.107888\n",
      "w8 : 0.164840\n",
      "RMSE : 1.866129\n",
      "\n",
      "\n",
      "Iterations : 1000\n",
      "Last Cost : 5.327325\n",
      "Second Last Cost : 5.327598 \n",
      "Learning rate : 0.000010 \n",
      "w0 : 0.040678 \n",
      "w1 : 0.011536\n",
      "w2 : 0.022634\n",
      "w3 : 0.001383 \n",
      "w4 : 0.000905\n",
      "w5 : 0.001604\n",
      "w6 : 0.000073 \n",
      "w7 : 0.012612\n",
      "w8 : 0.019155\n",
      "RMSE : 2.308100\n",
      "\n",
      "\n",
      "Iterations : 1000\n",
      "Last Cost : 5.581297\n",
      "Second Last Cost : 5.581326 \n",
      "Learning rate : 0.000001 \n",
      "w0 : 0.004130 \n",
      "w1 : 0.001168\n",
      "w2 : 0.002298\n",
      "w3 : 0.000140 \n",
      "w4 : 0.000092\n",
      "w5 : 0.000163\n",
      "w6 : 0.000007 \n",
      "w7 : 0.001282\n",
      "w8 : 0.001945\n",
      "RMSE : 2.362477\n",
      "\n",
      "\n",
      "Iterations : 1000\n",
      "Last Cost : 5.607556\n",
      "Second Last Cost : 5.607559 \n",
      "Learning rate : 0.000000 \n",
      "w0 : 0.000414 \n",
      "w1 : 0.000117\n",
      "w2 : 0.000230\n",
      "w3 : 0.000014 \n",
      "w4 : 0.000009\n",
      "w5 : 0.000016\n",
      "w6 : 0.000001 \n",
      "w7 : 0.000128\n",
      "w8 : 0.000195\n",
      "RMSE : 2.368028\n",
      "\n",
      "\n",
      "Iterations : 1000\n",
      "Last Cost : 5.610483\n",
      "Second Last Cost : 5.610483 \n",
      "Learning rate : 0.000000 \n",
      "w0 : 0.000000 \n",
      "w1 : 0.000000\n",
      "w2 : 0.000000\n",
      "w3 : 0.000000 \n",
      "w4 : 0.000000\n",
      "w5 : 0.000000\n",
      "w6 : 0.000000 \n",
      "w7 : 0.000000\n",
      "w8 : 0.000000\n",
      "RMSE : 2.368646\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for rate  in learning_rates:\n",
    "    W_mat = np.asmatrix(np.array([0., 0., 0., 0., 0., 0., 0., 0., 0.])).T\n",
    "    count = 0\n",
    "    max_count = 1000\n",
    "    current_cost = calculate_cost()\n",
    "    new_cost = 0\n",
    "    while new_cost < current_cost and count < max_count:\n",
    "        current_cost = calculate_cost()\n",
    "        calculate_weights(rate)\n",
    "        new_cost = calculate_cost()\n",
    "        count += 1\n",
    "    print \"Iterations : %d\" % count\n",
    "    print \"Last Cost : %lf\" % new_cost\n",
    "    print \"Second Last Cost : %f \" % current_cost \n",
    "    print \"Learning rate : %f \" % rate\n",
    "    print \"w0 : %f \" % W_mat[0]\n",
    "    print \"w1 : %f\" % W_mat[1]\n",
    "    print \"w2 : %f\" % W_mat[2]\n",
    "    print \"w3 : %f \" % W_mat[3]\n",
    "    print \"w4 : %f\" % W_mat[4]\n",
    "    print \"w5 : %f\" % W_mat[5]\n",
    "    print \"w6 : %f \" % W_mat[6]\n",
    "    print \"w7 : %f\" % W_mat[7]\n",
    "    print \"w8 : %f\" % W_mat[8]\n",
    "    print \"RMSE : %f\" % sqrt(calculate_cost())\n",
    "    print \"\\n\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Best Result with Learning rate : 0.10000 \n",
    "\n",
    "Iterations : 1000 <br />\n",
    "Last Cost : 0.579215 <br />\n",
    "Second Last Cost : 0.579251 <br /> \n",
    "Learning rate : 0.100000 <br />\n",
    "w0 : 1.703980 <br />\n",
    "w1 : 5.861136<br />\n",
    "w2 : 0.776703<br />\n",
    "w3 : 0.154956 <br />\n",
    "w4 : 0.072752<br />\n",
    "w5 : 0.142323<br />\n",
    "w6 : -0.063152 <br />\n",
    "w7 : -1.788810<br />\n",
    "w8 : -1.780212<br />\n",
    "RMSE : 0.761062<br />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3-2. Linear regression with multiple variables using Normal equation\n",
    "Fit the data (ex1data3.csv) using Normal equation. You need to calculate cost function and update weight.\n",
    "Please print the best Root Mean Squared Error (RMSE) after optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "X1 = data[0]\n",
    "X2 = data[1]\n",
    "X3 = data[2]\n",
    "X4 = data[3]\n",
    "X5 = data[4]\n",
    "X6 = data[5]\n",
    "X7 = data[6]\n",
    "X8 = data[7]\n",
    "Y = data[8]\n",
    "m = len(data)\n",
    "\n",
    "X1 = X1.astype(float) \n",
    "X2 = X2.astype(float)\n",
    "X3 = X3.astype(float) \n",
    "X4 = X4.astype(float)\n",
    "X5 = X5.astype(float) \n",
    "X6 = X6.astype(float)\n",
    "X7 = X7.astype(float) \n",
    "X8 = X8.astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "X1_min = X1.min()\n",
    "X2_min = X2.min()\n",
    "X3_min = X3.min()\n",
    "X4_min = X4.min()\n",
    "X5_min = X5.min()\n",
    "X6_min = X6.min()\n",
    "X7_min = X7.min()\n",
    "X8_min = X8.min()\n",
    "X1_max = X1.max()\n",
    "X2_max = X2.max()\n",
    "X3_max = X3.max()\n",
    "X4_max = X4.max()\n",
    "X5_max = X5.max()\n",
    "X6_max = X6.max()\n",
    "X7_max = X7.max()\n",
    "X8_max = X8.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "for x1,x2,x3,x4,x5,x6,x7,x8 in zip(X1,X2,X3,X4,X5,X6,X7,X8):\n",
    "    X1[i] = ((x1 - X1_min)/(X1_max - X1_min))\n",
    "    X2[i] = ((x2 - X2_min)/(X2_max - X2_min))\n",
    "    X3[i] = ((x3 - X3_min)/(X3_max - X3_min))\n",
    "    X4[i] = ((x4 - X4_min)/(X4_max - X4_min))\n",
    "    X5[i] = ((x5 - X5_min)/(X5_max - X5_min))\n",
    "    X6[i] = ((x6 - X6_min)/(X6_max - X6_min))\n",
    "    X7[i] = ((x7 - X7_min)/(X7_max - X7_min))\n",
    "    X8[i] = ((x8 - X8_min)/(X8_max - X8_min))\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = pd.Series( (1 for i in range(0,len(data))) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_mat = np.asmatrix((np.column_stack((temp,X1,X2,X3,X4,X5,X6,X7,X8))))\n",
    "W_mat = np.asmatrix(np.array([0., 0., 0., 0., 0., 0., 0., 0., 0.])).T\n",
    "Y_mat = np.asmatrix((np.row_stack((Y))))\n",
    "learning_rates = [0.1,0.01,0.001,0.0001,0.00001,0.000001,0.0000001,0.00000000001]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weight_function():\n",
    "    global Y_mat\n",
    "    result = np.matmul(np.matmul(np.matmul(X_mat.T,X_mat).I,X_mat.T),Y_mat)\n",
    "    return result\n",
    "def predict_Y(theta):\n",
    "    result = np.matmul(X_mat,theta)\n",
    "    return result\n",
    "def calculate_cost(Y_pred):\n",
    "    result = 0\n",
    "    for i in range(0,len(Y_pred)):\n",
    "        result += (Y_pred[i] - Y_mat[i])**2\n",
    "    result /=m\n",
    "    return result   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta = weight_function()\n",
    "Y_pred = predict_Y(theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w0 : 3.729612 \n",
      "w1 : 6.332140\n",
      "w2 : 0.481225\n",
      "w3 : -15.139162\n",
      "w4 : 21.760216\n",
      "w5 : -0.141874\n",
      "w6 : -4.705313\n",
      "w7 : -3.964568\n",
      "w8 : -4.362518\n",
      "RMSE : 0.724100\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print \"w0 : %f \" % theta[0]\n",
    "print \"w1 : %f\" % theta[1]\n",
    "print \"w2 : %f\" % theta[2]\n",
    "print \"w3 : %f\" % theta[3]\n",
    "print \"w4 : %f\" % theta[4]\n",
    "print \"w5 : %f\" % theta[5]\n",
    "print \"w6 : %f\" % theta[6]\n",
    "print \"w7 : %f\" % theta[7]\n",
    "print \"w8 : %f\" % theta[8]\n",
    "print \"RMSE : %f\" % sqrt(calculate_cost(Y_pred))\n",
    "print \"\\n\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Result of Normal Equation:\n",
    "w0 : 3.729612 <br /> \n",
    "w1 : 6.332140 <br />\n",
    "w2 : 0.481225<br />\n",
    "w3 : -15.139162<br />\n",
    "w4 : 21.760216<br />\n",
    "w5 : -0.141874<br />\n",
    "w6 : -4.705313<br />\n",
    "w7 : -3.964568<br />\n",
    "w8 : -4.362518<br />\n",
    "RMSE : 0.724100<br />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear regression with multiple variables using scikit-learn linear regression model\n",
    "Fit the data (ex1data3.csv) using linear regression from scikit-learn library. You need to calculate cost function\n",
    "and update weight.\n",
    "Please print the best Root Mean Squared Error (RMSE) after optimization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Done a Min-Max Normalization using code from above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_model = LinearRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_model.fit(X_mat,Y_mat)\n",
    "result = my_model.predict(X_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE is 0.724100 \n"
     ]
    }
   ],
   "source": [
    "print \"RMSE is %f \" % sqrt(mean_squared_error(Y_mat,result))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Result of Scikit learn Linear Regression\n",
    "\n",
    "RMSE is : 0.724100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3-4. Linear regression with multiple variables using TensorFlow\n",
    "Fit the data (ex1data3.csv) using linear regression using TensorFlow. You need to normalize variables. You\n",
    "need to calculate cost function and update weight using gradient descent method instead of Normal equation.\n",
    "Please print the best Root Mean Squared Error (RMSE) after optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "housing = fetch_california_housing()\n",
    "m, n = housing.data.shape\n",
    "X = tf.constant(X_mat, dtype=tf.float32, name=\"X\")\n",
    "y = tf.constant(Y_mat, dtype=tf.float32, name=\"y\")\n",
    "XT = tf.transpose(X)\n",
    "theta = tf.matmul(tf.matmul(tf.matrix_inverse(tf.matmul(XT, X)), XT), y)\n",
    "with tf.Session() as sess:\n",
    "    theta_value = theta.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  3.72974086],\n",
       "       [  6.33249378],\n",
       "       [  0.48119265],\n",
       "       [-15.14693642],\n",
       "       [ 21.76514816],\n",
       "       [ -0.14191604],\n",
       "       [ -4.70538664],\n",
       "       [ -3.96461535],\n",
       "       [ -4.3626194 ]], dtype=float32)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "theta_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(X_mat.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Epoch', 0, 'RMSE =', 2.5471997)\n",
      "('Epoch', 100, 'RMSE =', 0.96276641)\n",
      "('Epoch', 200, 'RMSE =', 0.87829679)\n",
      "('Epoch', 300, 'RMSE =', 0.83100903)\n",
      "('Epoch', 400, 'RMSE =', 0.80427802)\n",
      "('Epoch', 500, 'RMSE =', 0.78879064)\n",
      "('Epoch', 600, 'RMSE =', 0.77936673)\n",
      "('Epoch', 700, 'RMSE =', 0.77321076)\n",
      "('Epoch', 800, 'RMSE =', 0.76884234)\n",
      "('Epoch', 900, 'RMSE =', 0.76548481)\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 1000\n",
    "learning_rate = 0.1\n",
    "X = tf.constant(X_mat, dtype=tf.float32, name=\"X\")\n",
    "y = tf.constant(Y_mat, dtype=tf.float32, name=\"y\")\n",
    "theta = tf.Variable(tf.random_uniform([X_mat.shape[1], 1], -1.0, 1.0), name=\"theta\")\n",
    "y_pred = tf.matmul(X, theta, name=\"predictions\")\n",
    "error = y_pred - y\n",
    "rmse = tf.sqrt(tf.reduce_mean(tf.square(error), name=\"rmse\"))\n",
    "gradients = 2/m * tf.matmul(tf.transpose(X), error)\n",
    "training_op = tf.assign(theta, theta - learning_rate * gradients)\n",
    "init = tf.global_variables_initializer()\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for epoch in range(n_epochs):\n",
    "        if epoch % 100 == 0:\n",
    "            print(\"Epoch\", epoch, \"RMSE =\", rmse.eval())\n",
    "        sess.run(training_op)\n",
    "    best_theta = theta.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Result of Tensor flow Gradient descent:\n",
    "Iterations : 1000 <br />\n",
    "\n",
    "Learning rate : 0.1 <br />\n",
    "\n",
    "RMSE : 0.76548481<br />"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
