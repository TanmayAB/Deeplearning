{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import functools\n",
    "import vgg\n",
    "import time\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import cv2\n",
    "import scipy\n",
    "import scipy.misc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_inst_relu(input_layer, num_filters, kernel_size, strides):\n",
    "\n",
    "    output_layer = tf.layers.conv2d(input_layer, num_filters, kernel_size, strides, padding='SAME')\n",
    "    output_layer = instance_norm(output_layer)\n",
    "    output_layer = tf.nn.relu(output_layer)\n",
    "\n",
    "    return output_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_trans_inst_relu(input_layer, num_filters, kernel_size, strides):\n",
    "\n",
    "    output_layer = tf.layers.conv2d_transpose(input_layer, num_filters, kernel_size, strides, padding='SAME')\n",
    "    output_layer = instance_norm(output_layer)\n",
    "    return tf.nn.relu(output_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def residual_block(input_layer, kernel_size=3):\n",
    "    \n",
    "    output_layer = conv_inst_relu(input_layer, 128, kernel_size, 1)\n",
    "    \n",
    "    return input_layer + conv_inst_relu(output_layer, 128, kernel_size, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Best model with Three convolutional layers,  Five residual blocks and Three Convolutional Transformation layeers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_pass(image):\n",
    "    \n",
    "    conv1 = conv_inst_relu(image, 32, 9, (1, 1))\n",
    "    conv2 = conv_inst_relu(conv1, 64, 3, (2, 2))\n",
    "    conv3 = conv_inst_relu(conv2, 128, 3, (2, 2))\n",
    "    \n",
    "    resi1 = residual_block(conv3, (3, 3))\n",
    "    resi2 = residual_block(resi1, (3, 3))\n",
    "    resi3 = residual_block(resi2, (3, 3))\n",
    "    resi4 = residual_block(resi3, (3, 3))\n",
    "    resi5 = residual_block(resi4, (3, 3))\n",
    "    \n",
    "    convT1 = conv_trans_inst_relu(resi5, 64, 3, (2, 2))\n",
    "    convT2 = conv_trans_inst_relu(convT1, 32, 3, (2, 2))\n",
    "    convT3 = conv_trans_inst_relu(convT2, 3, 9, 1)\n",
    "    \n",
    "    trans = tf.nn.tanh(convT3) * 150 + 255./2\n",
    "    \n",
    "    return trans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Longer network with additional convoluational layer and Conv Transofrmation layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def forward_pass(image):\n",
    "    \n",
    "#     conv1 = conv_inst_relu(image, 32, 9, (1, 1))\n",
    "#     conv2 = conv_inst_relu(conv1, 64, 3, (2, 2))\n",
    "#     conv3 = conv_inst_relu(conv2, 128, 3, (2, 2))\n",
    "#     conv4 = conv_inst_relu(conv3, 256, 3, (2, 2))\n",
    "    \n",
    "#     resi1 = residual_block(conv4, (3, 3))\n",
    "#     resi2 = residual_block(resi1, (3, 3))\n",
    "#     resi3 = residual_block(resi2, (3, 3))\n",
    "#     resi4 = residual_block(resi3, (3, 3))\n",
    "#     resi5 = residual_block(resi4, (3, 3))\n",
    "    \n",
    "#     convT1 = conv_trans_inst_relu(resi5, 128, 3, (2, 2))\n",
    "#     convT2 = conv_trans_inst_relu(convT1, 64, 3, (2, 2))\n",
    "#     convT3 = conv_trans_inst_relu(convT2, 32, 3, (2, 2))\n",
    "#     convT4 = conv_trans_inst_relu(convT3, 3, 9, 1)\n",
    "    \n",
    "#     trans = tf.nn.tanh(convT4) * 150 + 255./2\n",
    "    \n",
    "#     return trans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shorter network with only two convoluational layers, three residual blocks and Conv Transofrmation layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def forward_pass(image):\n",
    "    \n",
    "#     conv1 = conv_inst_relu(image, 32, 9, (1, 1))\n",
    "#     conv2 = conv_inst_relu(conv1, 64, 3, (2, 2))\n",
    "    \n",
    "#     resi1 = residual_block(conv2, (3, 3))\n",
    "#     resi2 = residual_block(resi1, (3, 3))\n",
    "#     resi3 = residual_block(resi2, (3, 3))\n",
    "    \n",
    "#     convT1 = conv_trans_inst_relu(resi3, 32, 3, (2, 2))\n",
    "#     convT2 = conv_trans_inst_relu(convT1, 3, 9, 1)\n",
    "    \n",
    "#     trans = tf.nn.tanh(convT2) * 150 + 255./2\n",
    "    \n",
    "#     return trans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalization functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def instance_norm(input_layer, train=True):\n",
    "    batch, rows, cols, ch = [i.value for i in input_layer.get_shape()]\n",
    "    mu, sigma_sq = tf.nn.moments(input_layer, [1,2], keep_dims=True)\n",
    "    shift = tf.Variable(tf.zeros([ch]))\n",
    "    scale = tf.Variable(tf.ones([ch]))\n",
    "    epsilon = 1e-3\n",
    "    normalized = (input_layer-mu)/(sigma_sq + epsilon)**(.5)\n",
    "    return scale * normalized + shift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_norm(input_layer, train=True):\n",
    "    ouput_layer = tf.layers.batch_normalization(input_layer, training=True)\n",
    "    return output_layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initializer functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def placeholder_initializer(style_shape, batch_shape):\n",
    "   \n",
    "    style_image = tf.placeholder(tf.float32, shape=style_shape, name='style_image')\n",
    "    X_content = tf.placeholder(tf.float32, shape=batch_shape, name=\"X_content\")\n",
    "\n",
    "    return style_image, X_content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss():\n",
    "   \n",
    "    global STYLE_TARGET\n",
    "\n",
    "    mod = len(CONTENT_TARGETS) % BATCH_SIZE\n",
    "    if mod > 0:\n",
    "        STYLE_TARGET = STYLE_TARGET[:-mod] \n",
    "\n",
    "    style_features = {}\n",
    "\n",
    "    batch_shape = (BATCH_SIZE,256,256,3)\n",
    "    style_shape = (1,) + STYLE_TARGET.shape\n",
    "    \n",
    "    with tf.Graph().as_default(), tf.device('/cpu:0'), tf.Session() as sess:\n",
    "        style_image, X_content = placeholder_initializer(style_shape, batch_shape)        \n",
    "        style_image_pre = vgg.preprocess(style_image)\n",
    "        net = vgg.net(VGG_PATH, style_image_pre)\n",
    "        style_pre = np.array([STYLE_TARGET])\n",
    "        for layer in STYLE_LAYERS:\n",
    "            features = net[layer].eval(feed_dict={style_image:style_pre})\n",
    "            features = np.reshape(features, (-1, features.shape[3]))\n",
    "            gram = np.matmul(features.T, features) / features.size\n",
    "            style_features[layer] = gram\n",
    "\n",
    "        X_pre = vgg.preprocess(X_content)\n",
    "\n",
    "        content_features = {}\n",
    "        content_net = vgg.net(VGG_PATH, X_pre)\n",
    "        content_features[CONTENT_LAYER] = content_net[CONTENT_LAYER]\n",
    "\n",
    "        preds = forward_pass(X_content/255.0)\n",
    "        preds_pre = vgg.preprocess(preds)\n",
    "\n",
    "        net = vgg.net(VGG_PATH, preds_pre)\n",
    "\n",
    "        tensor_shape = content_features[CONTENT_LAYER].get_shape()\n",
    "        mul = 1\n",
    "        for item in tensor_shape.as_list():\n",
    "            mul *= item\n",
    "        content_size = mul * BATCH_SIZE\n",
    "        content_loss = CONTENT_WEIGHT * (2 * tf.nn.l2_loss(\n",
    "            net[CONTENT_LAYER] - content_features[CONTENT_LAYER]) / content_size\n",
    "        )\n",
    "\n",
    "        style_losses = []\n",
    "        for style_layer in STYLE_LAYERS:\n",
    "            layer = net[style_layer]\n",
    "            bs, height, width, filters = map(lambda i:i.value,layer.get_shape())\n",
    "            size = height * width * filters\n",
    "            feats = tf.reshape(layer, (bs, height * width, filters))\n",
    "            feats_T = tf.transpose(feats, perm=[0,2,1])\n",
    "            grams = tf.matmul(feats_T, feats) / size\n",
    "            style_gram = style_features[style_layer]\n",
    "            style_losses.append(2 * tf.nn.l2_loss(grams - style_gram)/style_gram.size)\n",
    "\n",
    "        style_loss = STYLE_WEIGHT * functools.reduce(tf.add, style_losses) / BATCH_SIZE\n",
    "       \n",
    "        loss = content_loss + style_loss\n",
    "\n",
    "        train_step = tf.train.AdamOptimizer(LEARNING_RATE).minimize(loss)\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "\n",
    "        for epoch in range(EPOCHS):\n",
    "            num_examples = len(CONTENT_TARGETS)\n",
    "            counts = 0\n",
    "            while counts * BATCH_SIZE < num_examples:\n",
    "                start_time = time.time()\n",
    "                curr = counts * BATCH_SIZE\n",
    "                step = curr + BATCH_SIZE\n",
    "                X_batch = np.zeros(batch_shape, dtype=np.float32)\n",
    "                for j, img_p in enumerate(CONTENT_TARGETS[curr:step]):\n",
    "                    \n",
    "                    STYLE_TARGET = cv2.imread(STYLE_PATH)\n",
    "                    STYLE_TARGET = cv2.cvtColor(STYLE_TARGET,cv2.COLOR_BGR2RGB)\n",
    "                    STYLE_TARGET = cv2.resize(STYLE_TARGET,(256,256))\n",
    "                    X_batch[j] = STYLE_TARGET.astype(np.float32)\n",
    "\n",
    "                counts += 1\n",
    "                \n",
    "                train_step.run(feed_dict={X_content:X_batch})\n",
    "                end_time = time.time()\n",
    "                delta_time = end_time - start_time\n",
    "                                \n",
    "                is_last = epoch == EPOCHS - 1 and counts * BATCH_SIZE >= num_examples\n",
    "                \n",
    "                if is_last:\n",
    "                    to_get = [style_loss, content_loss, loss, preds]\n",
    "                    test_feed_dict = {\n",
    "                       X_content:X_batch\n",
    "                    }\n",
    "\n",
    "                    tup = sess.run(to_get, feed_dict = test_feed_dict)\n",
    "                    _style_loss,_content_loss,_loss,_preds = tup\n",
    "                    losses = (_style_loss, _content_loss, _loss)\n",
    "                    \n",
    "                    saver = tf.train.Saver()\n",
    "                    res = saver.save(sess, MODEL_SAVE_PATH)\n",
    "                    print('Model successfully saved at : ', str(MODEL_SAVE_PATH + 'fns.ckpt'))\n",
    "                    yield(epoch, counts, _preds, losses) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Declaring Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "STYLE_LAYERS = ('relu1_1', 'relu2_1', 'relu3_1', 'relu4_1', 'relu5_1')\n",
    "CONTENT_LAYER = 'relu4_2'\n",
    "\n",
    "CONTENT_WEIGHT = 7.5e0\n",
    "STYLE_WEIGHT = 1e2\n",
    "\n",
    "LEARNING_RATE = 1e-3\n",
    "EPOCHS = 2\n",
    "BATCH_SIZE = 4\n",
    "\n",
    "MODEL_SAVE_PATH = './saved_model/fns.ckpt'\n",
    "VGG_PATH = './imagenet-vgg-verydeep-19.mat'\n",
    "TRAIN_PATH = './train_sample/'\n",
    "STYLE_PATH = './style/wave.jpg'\n",
    "\n",
    "TEST_IMAGE_PATH = './test_images/batsman.jpg'\n",
    "OUTPUT_PATH = './output/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "files_list = []\n",
    "for filename in os.listdir(TRAIN_PATH):\n",
    "    if filename.endswith(\".DS_Store\"):\n",
    "        A = 1\n",
    "    else:\n",
    "        files_list.append(filename)  \n",
    "\n",
    "CONTENT_TARGETS = [os.path.join(TRAIN_PATH,x) for x in files_list]\n",
    "\n",
    "\n",
    "STYLE_TARGET= cv2.imread(STYLE_PATH)\n",
    "STYLE_TARGET = cv2.cvtColor(STYLE_TARGET,cv2.COLOR_BGR2RGB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model successfully saved at :  ./saved_model/fns.ckptfns.ckpt\n",
      "Epoch :  1\n",
      "Iterations:  4\n",
      "Content Loss :  880425.44\n",
      "Style Loss :  49640750.0\n",
      "Overall Loss :  50521176.0\n",
      "************\n"
     ]
    }
   ],
   "source": [
    "for epoch, counts, _preds, losses in loss():\n",
    "    style_loss, content_loss, loss = losses\n",
    "\n",
    "    print('Epoch : ',epoch)\n",
    "    print('Iterations: ', counts)\n",
    "    print('Content Loss : ', content_loss)\n",
    "    print('Style Loss : ', style_loss)\n",
    "    print('Overall Loss : ', loss)\n",
    "    print(\"************\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./test_images/batsman.jpg\n"
     ]
    }
   ],
   "source": [
    "print(TEST_IMAGE_PATH)\n",
    "in_img = cv2.imread(TEST_IMAGE_PATH)\n",
    "in_img = cv2.cvtColor(in_img, cv2.COLOR_BGR2RGB)\n",
    "in_img.shape = (1,) + in_img.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform Image style transfer using trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./saved_model/fns.ckpt\n",
      "Transformed stored at :  ./output/transformed_image.jpg\n"
     ]
    }
   ],
   "source": [
    "with tf.Graph().as_default(), tf.Session() as sess:\n",
    "    \n",
    "    img = tf.placeholder(tf.float32, shape=in_img.shape,name='input_image')\n",
    "    \n",
    "    trans = forward_pass(img)\n",
    "    saver = tf.train.Saver()\n",
    "    saver.restore(sess, str(MODEL_SAVE_PATH))\n",
    "    \n",
    "    transformed_image_before_reshape = sess.run(trans, feed_dict={img:in_img}) \n",
    "    transformed_image = transformed_image_before_reshape[0]\n",
    "    cv2.imwrite(OUTPUT_PATH + 'transformed_image.jpg', transformed_image)\n",
    "    print(\"Transformed stored at : \", OUTPUT_PATH + \"transformed_image.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
